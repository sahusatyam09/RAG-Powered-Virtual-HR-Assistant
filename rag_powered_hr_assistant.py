# -*- coding: utf-8 -*-
"""RAG-Powered HR Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xj1R7dly5M_MAlevRBf6PEmTe1cqn9n3
"""

!pip install --quiet sentence-transformers==2.2.2
!pip install --quiet transformers==4.33.2
!pip install --quiet huggingface_hub==0.17.3
!pip install --quiet langchain-huggingface==0.1.2

!pip install langchain-community
!pip install pypdf
from langchain_community.document_loaders import PyPDFLoader
# Load the PDF
loader = PyPDFLoader("/content/Synise Handbook.pdf")
pages = loader.load()

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
docs = text_splitter.split_documents(pages)

!pip install chromadb
from langchain.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Create or load Chroma vector store
db = Chroma.from_documents(docs, embedding_model, persist_directory="chromadb")
db.persist()
retriever = db.as_retriever()

# STEP-5 âœ…  model Falcon-7B

from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from transformers import pipeline

falcon_pipeline = pipeline(
    "text-generation",
    model="tiiuae/falcon-7b-instruct",
    max_new_tokens=150,
    temperature=0.3,
    repetition_penalty=1.2,
    trust_remote_code=True,
    device=0,
)

llm = HuggingFacePipeline(pipeline=falcon_pipeline)

custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an experienced HR assistant. Use only the context below from the employee handbook to answer accurately.
DO NOT make up answers. If you don't know the answer, just say that you don't know. Don't try to make up an answer.

Context:
{context}

Question: {question}
Answer:"""
)

qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)

from langchain.embeddings import HuggingFaceEmbeddings
import numpy as np

def answer_query(question):
    try:
        retrieved_docs = retriever.get_relevant_documents(question)

        # Filter 1: No documents found
        if not retrieved_docs:
            return "I don't know. This information is not available in the uploaded document."

        # Filter 2: Short irrelevant documents
        if all(len(doc.page_content.strip()) < 30 for doc in retrieved_docs):
            return "This is sounding like a vague question.Please elaborate it more !"

        # Optional semantic similarity filtering
        embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
        query_emb = embedding_model.embed_query(question)

        def cosine_similarity(a, b):
            return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

        doc_scores = [cosine_similarity(query_emb, embedding_model.embed_query(doc.page_content)) for doc in retrieved_docs]

        if max(doc_scores) < 0.55:
            return "This is sounding like a vague question.Please elaborate it more !"

        # Run model if content seems relevant
        return qa.run(question).strip()

    except Exception as e:
        return f"Error: {str(e)}"

# Example 1: In-context question
#print("Q: What is the leave policy for sick days?")
#print("A:", answer_query("What is the leave policy for sick days?"))

# Example 2: Out-of-context question
#print("\nQ: How to start a bike?")
#print("A:", answer_query("How to start a bike?"))

# Example 3: Vague question
#print("\nQ: policy for travel allowances?")
#print("A:", answer_query("policy for travel allowances?"))

!pip install gradio

def ask_hr_bot(question):
    # This will call your RAG pipeline to get the answer
    return rag_pipeline.run(question)

import gradio as gr

iface = gr.Interface(
    fn=ask_hr_bot,        # The function that gets the answer
    inputs="text",        # User will type a question
    outputs="text",       # Output will be plain text
    title="HR Policy Assistant"
)

iface.launch()